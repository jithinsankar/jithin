## Regression

**Regression** is a statistical method used to understand the relationship between a dependent variable and one or more independent variables. It helps in predicting the dependent variable based on the values of the independent variables. Regression is widely used in various fields such as finance, economics, biology, and machine learning to model and analyze data.

**Linear Regression** is the simplest form of regression. It assumes a linear relationship between the dependent variable (often denoted as \(y\)) and one or more independent variables (denoted as \(x\)). The relationship can be expressed by the equation:

\[
y = \beta_0 + \beta_1x + \epsilon
\]

Where:

- \(y\) is the dependent variable.
- \(x\) is the independent variable.
- \(\beta_0\) is the y-intercept (the value of \(y\) when \(x\) is 0).
- \(\beta_1\) is the slope of the line (how much \(y\) changes for a unit change in \(x\)).
- \(\epsilon\) represents the error term, accounting for the variability in \(y\) that cannot be explained by the linear relationship.

The goal of linear regression is to find the best-fitting line through the data points that minimizes the sum of the squared differences between the observed values and the values predicted by the line. This method is known as **least squares**.

Linear regression is useful for predicting outcomes and understanding the strength and direction of relationships between variables.
